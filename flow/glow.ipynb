{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60cef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waj/anaconda3/envs/tts/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1 import package\n",
    "import torch\n",
    "import  torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg as la\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03191991",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "EPOCHS=200\n",
    "image_size=32\n",
    "channel=1\n",
    "z_dim=128\n",
    "n_flow=30\n",
    "n_block=4\n",
    "n_channel=1\n",
    "n_sampls=25\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f006292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jitter:\n",
    "    \"\"\"Transform for dataloader, adds uniform jitter noise to data\"\"\"\n",
    "\n",
    "    def __init__(self, scale=1.0 / 256):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "          scale: Scaling factor for noise\n",
    "        \"\"\"\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        eps = torch.rand_like(x) * self.scale\n",
    "        x_ = x + eps\n",
    "        return x_\n",
    "    \n",
    "class Scale:\n",
    "    \"\"\"Transform for dataloader, adds uniform jitter noise to data\"\"\"\n",
    "\n",
    "    def __init__(self, scale=255.0 / 256.0):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "          scale: Scaling factor for noise\n",
    "        \"\"\"\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7360efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mnist_image(image_array, label):\n",
    "    plt.imshow(image_array, cmap='Greys')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "# 1 data loader \n",
    "dataset=datasets.MNIST(\"../data/\",train=True,transform=transforms.Compose([\n",
    "    transforms.Resize(32),transforms.ToTensor()\n",
    "]))\n",
    "mnist=DataLoader(dataset,shuffle=True,batch_size=BATCH_SIZE,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab469bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in mnist:\n",
    "    print(data[0].shape)\n",
    "    show_mnist_image(data[0][0][0], data[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4613d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_shapes(n_channel,input_size,n_flow,n_block):\n",
    "    z_shapes=[]\n",
    "    for i in range(n_block-1):\n",
    "        input_size//=2\n",
    "        n_channel*=2\n",
    "        z_shapes.append((n_channel,input_size,input_size))\n",
    "    input_size//=2\n",
    "    z_shapes.append((n_channel*4,input_size,input_size))\n",
    "    return z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98084eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 16, 16), (4, 8, 8), (8, 4, 4), (32, 2, 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_shapes=calc_z_shapes(n_channel,image_size,n_flow,n_block)\n",
    "z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c15daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2, 16, 16])\n",
      "torch.Size([25, 4, 8, 8])\n",
      "torch.Size([25, 8, 4, 4])\n",
      "torch.Size([25, 32, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_samples=[]\n",
    "for z in z_shapes:\n",
    "    z_new=torch.randn(n_sampls,*z)\n",
    "    z_samples.append(z_new)\n",
    "[print(x.shape) for x in z_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382bc40",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608929d1",
   "metadata": {},
   "source": [
    "![架构图](./architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logabs=lambda x : torch.log(torch.abs(x))\n",
    "class Actnorm(nn.Module):\n",
    "    def __init__(self,in_channel,logdet=True):\n",
    "        super(Actnorm,self).__init__()\n",
    "        self.bias=nn.Parameter(torch.zeros(1,in_channel,1,1))\n",
    "        self.scale=nn.Parameter(torch.ones(1,in_channel,1,1))\n",
    "        self.register_buffer(\"initialized\",torch.tensor(0))\n",
    "        self.logdet=logdet\n",
    "    \n",
    "    # 求出channel维度的均值和方差作为初始值\n",
    "    def initialize(self,input):\n",
    "        with torch.no_grad():\n",
    "            flatten=input.permute(1,0,2,3).contiguous().view(input.shape[1],-1)\n",
    "            mean=flatten.mean(1).unsqueeze(1).unsqueeze(2).unsqueeze(3).permute(1,0,2,3)\n",
    "            std=flatten.std(1).unsqueeze(1).unsqueeze(2).unsqueeze(3).permute(1,0,2,3)\n",
    "            self.bias.data.copy_(-mean)\n",
    "            self.scale.data.copy_(1/(std+1e-6)) # 防止数据下益\n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        if self.initialized.item()==0:\n",
    "            self.initialize(input)\n",
    "            self.initialized.fill_(1)\n",
    "        \n",
    "        y=(input+self.bias)*self.scale\n",
    "        \n",
    "        if self.logdet:\n",
    "            logdet=height*width*torch.sum(logabs(self.scale))\n",
    "            return y,logdet\n",
    "        else :\n",
    "            return y\n",
    "        \n",
    "    def reverse(self,input):\n",
    "        return (input-self.bias)/self.scale\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0407787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 28, 28]) tensor(1.9927, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "actorn=Actnorm(1)\n",
    "x=torch.randn(3,1,28,28)\n",
    "x,logdet=actorn(x)\n",
    "print(x.shape,logdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad88d3d",
   "metadata": {},
   "source": [
    "![架构图](./table3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1942a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvConv2d(nn.Module):\n",
    "    def __init__(self,in_channel):\n",
    "        super(InvConv2d,self).__init__()\n",
    "        self.weight=nn.Parameter(torch.randn(in_channel,in_channel,1,1))\n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        out=nn.functional.conv2d(input,self.weight)\n",
    "        logdet=height*width*torch.slogdet(self.weight.double())[1].float()\n",
    "        return out ,logdet\n",
    "    def reverse(self,input):\n",
    "        out=nn.functional.conv2d(input,self.weight.inverse().unsqueeze().unsqueeze(3))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec77b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46189072469327913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.81428037,  0.48849559,  0.31355946],\n",
       "        [-0.53774939, -0.83822175, -0.09060846],\n",
       "        [ 0.21857053, -0.2423971 ,  0.94523572]]),\n",
       " array([[-1.46753404, -2.05675499, -0.6560951 ],\n",
       "        [ 0.        ,  0.25772116, -0.73058326],\n",
       "        [ 0.        ,  0.        ,  1.58161801]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight=np.random.randn(3,3)\n",
    "q,d=la.qr(weight)\n",
    "print(sum(q[1,:]*q[:,1]))\n",
    "q,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9d434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81428037  0.48849559  0.31355946]\n",
      " [ 0.         -1.16082342 -0.2976826 ]\n",
      " [ 0.          0.          1.05793717]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.81428037, -1.16082342,  1.05793717])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,l,u=la.lu(q)\n",
    "print(u)\n",
    "np.diag(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5840522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.48849559,  0.31355946],\n",
       "       [ 0.        ,  0.        , -0.2976826 ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(u,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4fef4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones_like(u),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e62e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_weight=np.random.randn(3,3)\n",
    "q,r=la.qr(r_weight)\n",
    "p,l,u=la.lu(q.astype(np.float32))\n",
    "s=np.diag(u)\n",
    "u=np.triu(u,1)\n",
    "\n",
    "u_mask=np.triu(np.ones_like(u),1)\n",
    "l_mask=u_mask.T\n",
    "l_eye=np.eye(l_mask.shape[0])\n",
    "# weight=p@(l*l_mask+l_eye)@(u*u_mask+torch.diag(s_sign*torch.exp(s)))\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de4ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.21139191 -0.47241664]\n",
      " [ 0.          0.          0.27314025]\n",
      " [ 0.          0.          0.        ]] [0.42500708 0.38102576 3.3576167 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l*l_mask+l_eye\n",
    "print(u*u_mask,np.exp(s))\n",
    "u*u_mask+np.diag(np.exp(s))\n",
    "np.diag(np.exp(s)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6b9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvConv2d_lu(nn.Module):\n",
    "    def __init__(self,in_channel):\n",
    "        super(InvConv2d_lu,self).__init__()\n",
    "        weight=np.random.randn(in_channel,in_channel)\n",
    "        q,r=la.qr(weight)\n",
    "        p,l,u=la.lu(q.astype(np.float32))\n",
    "        s=np.diag(u)\n",
    "        u=np.triu(u,1)\n",
    "        u_mask=np.triu(np.ones_like(u),1)\n",
    "        l_mask=u_mask.T\n",
    "        \n",
    "        p=torch.from_numpy(p)\n",
    "        l=torch.from_numpy(l)\n",
    "        s=torch.from_numpy(s)\n",
    "        u=torch.from_numpy(u)\n",
    "        u_mask=torch.from_numpy(u_mask)\n",
    "        l_mask=torch.from_numpy(l_mask)\n",
    "        \n",
    "        self.register_buffer(\"p\",p)\n",
    "        self.register_buffer(\"u_mask\",u_mask)\n",
    "        self.register_buffer(\"l_mask\",l_mask)\n",
    "        self.register_buffer(\"s_sign\",torch.sign(s))\n",
    "        self.register_buffer(\"l_eye\",torch.eye(l_mask.shape[0]))\n",
    "        self.l=nn.Parameter(l)\n",
    "        self.s=nn.Parameter(logabs(s))\n",
    "        self.u=nn.Parameter(u)\n",
    "        \n",
    "    def calc_weight(self):\n",
    "        \n",
    "        weight=self.p@(self.l*self.l_mask+self.l_eye)@ \\\n",
    "        (self.u*self.u_mask+torch.diag(self.s_sign*torch.exp(self.s)))\n",
    "        return weight.unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        weight=self.calc_weight()\n",
    "        out=nn.functional.conv2d(input,weight)\n",
    "        logdet=height*width*torch.sum(self.s)\n",
    "        return out ,logdet\n",
    "    \n",
    "    def reverse(self,input):\n",
    "        weight=self.calc_weight()\n",
    "        out=nn.functional.conv2d(input,weight.squeeze().inverse().unsqueeze(2).unsqueeze(3)) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c542e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 28, 28]) tensor(0., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56041/2649664392.py:14: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  s=torch.from_numpy(s)\n"
     ]
    }
   ],
   "source": [
    "invConv_lu=InvConv2d_lu(1)\n",
    "\n",
    "x,logdet=invConv_lu(x)\n",
    "print(x.shape,logdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39567d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invConv_lu.u_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06da9bc",
   "metadata": {},
   "source": [
    "![架构图](./table3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a4e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroConv2d(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,padding=1):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv2d(in_channel,out_channel,3,padding)\n",
    "        self.conv.weight.data.zero_()\n",
    "        self.conv.bias.data.zero_()\n",
    "        self.scale=nn.Parameter(torch.zeros(1,out_channel,1,1))\n",
    "    def forward(self,input):\n",
    "        out=nn.functional.pad(input,[1,1,1,1],value=1)\n",
    "        out=self.conv(out)\n",
    "        out=out*torch.exp(self.scale*3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f90222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroConv=ZeroConv2d(1,10)\n",
    "zeroConv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6501641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self,in_channel,filter_size=512,affine=True):\n",
    "        super(AffineCoupling,self).__init__()\n",
    "        self.affine=affine\n",
    "        self.net=nn.Sequential(\n",
    "        nn.Conv2d(in_channel//2,filter_size,3,padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(filter_size,filter_size,1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        ZeroConv2d(filter_size,in_channel if self.affine else in_channel//2)\n",
    "        )\n",
    "        \n",
    "        self.net[0].weight.data.normal_(0,0.05)\n",
    "        self.net[0].bias.data.zero_()\n",
    "        self.net[2].weight.data.normal_(0,0.05)\n",
    "        self.net[2].bias.data.zero_()     \n",
    "    def forward(self,input):\n",
    "        in_a,in_b=input.chunk(2,1)\n",
    "        if self.affine:\n",
    "            logs,t=self.net(in_b).chunk(2,1)\n",
    "            y_a=torch.exp(logs)*in_a+t\n",
    "            y_b=in_b\n",
    "            logdet=torch.sum(logabs(torch.exp(logs)))\n",
    "        else:\n",
    "            y_b=in_b\n",
    "            y_a=in_a+self.net(in_b)\n",
    "            logdet=None\n",
    "#         return y_a,y_b\n",
    "        return torch.cat([y_a,y_b],1),logdet\n",
    "    def reverse(self,output):\n",
    "        y_a,y_b=output.chunk(2,1)\n",
    "        if self.affine:\n",
    "            in_b=y_b\n",
    "            logs,t=self.net(in_b).chunk(2, 1)\n",
    "            in_a=(y_a-t)/torch.exp(logs)\n",
    "        else :\n",
    "            in_b=y_b\n",
    "            in_a=self.net(in_b)-y_a\n",
    "        return torch.cat([in_a,in_b],1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac2765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 28, 28]), tensor(0., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affCoup=AffineCoupling(4)\n",
    "a,b=affCoup(torch.randn(3,4,28,28))\n",
    "\n",
    "a.shape,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41428b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2581, -0.6020])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.randn(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f802b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self,in_channel,affine=True,conv_lu=True):\n",
    "        super(Flow,self).__init__()\n",
    "        self.actorm=Actnorm(in_channel)\n",
    "        if conv_lu:\n",
    "            self.invConv2d=InvConv2d_lu(in_channel)\n",
    "        else:\n",
    "            self.invConv2d=InvConv2d(in_channel)\n",
    "        self.coupling=AffineCoupling(in_channel,256)\n",
    "    def forward(self,input):\n",
    "        out,det1=self.actorm(input)\n",
    "        out,det2=self.invConv2d(out)\n",
    "        out,det3=self.coupling(out)\n",
    "        logdet=det1+det2\n",
    "        if det3 is not None:\n",
    "            logdet+=det3\n",
    "        return out,logdet\n",
    "    \n",
    "    def reverse(self,output):\n",
    "        input_=self.coupling.reverse(output)\n",
    "        input_=self.invConv2d.reverse(input_)\n",
    "        input_=self.actorm.reverse(input_)\n",
    "        return input_\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "513d92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 14, 14]), tensor(1.2851, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow=Flow(4)\n",
    "a,b=flow(torch.randn(3,4,14,14))\n",
    "a.shape,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb9303",
   "metadata": {},
   "source": [
    "![架构图](./architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be752def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_p(x,mean,log_std):\n",
    "    return -0.5*math.log(2* math.pi)-log_std-0.5*(x-mean)**2/torch.exp(2*log_std)\n",
    "\n",
    "\n",
    "def gaussian_sample(eps,mean,log_std):\n",
    "    return mean+torch.exp(log_std)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "928fdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,in_channel,n_flow,split=True,affine=True,conv_lu=True):\n",
    "        super().__init__()\n",
    "        squeeze_dim=in_channel*4\n",
    "        self.flows=nn.ModuleList()\n",
    "        for i in range(n_flow):\n",
    "            self.flows.append(Flow(squeeze_dim,affine=affine,conv_lu=conv_lu))\n",
    "        self.split=split\n",
    "        \n",
    "        if split:\n",
    "            self.prior=ZeroConv2d(in_channel*2,in_channel*4)\n",
    "        else: \n",
    "            self.prior=ZeroConv2d(in_channel*4,in_channel*8)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        bs,n_channel,height,width=input.shape\n",
    "        squeezed=input.view(bs,n_channel,height//2,2,width//2,2)\n",
    "        squeezed=squeezed.permute(0,1,3,5,2,4)\n",
    "        out=squeezed.contiguous().view(bs,n_channel*4,height//2,width//2)\n",
    "        logdet=0\n",
    "\n",
    "        for flow in self.flows:\n",
    "            out,det=flow(out)\n",
    "            logdet+=det\n",
    "        \n",
    "        if self.split:\n",
    "            out,z_new=out.chunk(2,1)\n",
    "            mean,logstd=self.prior(out).chunk(2,1)\n",
    "            log_p=gaussian_log_p(out,mean,logstd)\n",
    "            log_p=log_p.view(bs,-1).sum(1)\n",
    "            \n",
    "        else:\n",
    "            zero=torch.zeros_like(out)\n",
    "            mean,logstd=self.prior(zero).chunk(2,1)\n",
    "            log_p=gaussian_log_p(out,mean,logstd)\n",
    "            log_p=log_p.view(bs,-1).sum(1)\n",
    "            z_new=out\n",
    "        return out,logdet,log_p,z_new\n",
    "    \n",
    "    def reverse(self,output,eps=None,reconstruct=False):\n",
    "        input=output\n",
    "        \n",
    "        if reconstruct:\n",
    "            if self.split:\n",
    "                input=torch.cat([output,eps],1)\n",
    "            else:\n",
    "                input=eps\n",
    "        else:\n",
    "            if self.split:\n",
    "                mean,logstd=self.prior(input).chunk(2,1)\n",
    "                z=gaussian_sample(eps,mean,logstd)\n",
    "                input=torch.cat([output,z],1)\n",
    "                \n",
    "            else:\n",
    "                zero=torch.zeros_like(input)\n",
    "                mean,logstd=self.prior(zero).chunk(2,1)\n",
    "                z=gaussian_sample(eps,mean,logstd)\n",
    "                input=z\n",
    "        for flow in self.flows[::-1]:\n",
    "            input=flow.reverse(input)\n",
    "            \n",
    "        bs,n_channel,height,width=input.shape\n",
    "        unsqueezed=input.view(bs,n_channel//4,2,2,height,width)\n",
    "        unsqueezed=unsqueezed.permute(0,1,4,2,5,3)\n",
    "        unsqueezed=unsqueezed.contiguous().view(bs,n_channel//4,height*2,width*2)\n",
    "        \n",
    "        return unsqueezed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f56f0eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 14, 14]),\n",
       " tensor(7.0470, grad_fn=<AddBackward0>),\n",
       " tensor([-546.8235, -548.6953, -573.1659], grad_fn=<SumBackward1>),\n",
       " torch.Size([3, 2, 14, 14]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block=Block(1,2)\n",
    "out,logdet,log_p,z_new=block(torch.randn(3,1,28,28))\n",
    "out.shape,logdet,log_p,z_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f81b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior=ZeroConv2d(1*2,1*4)\n",
    "prior(torch.zeros(1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64cbc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glow(nn.Module):\n",
    "    def __init__(self,in_channel,n_flow,n_block,affine=True,conv_lu=True):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList()\n",
    "        n_channel=in_channel\n",
    "        for i in range(n_block-1):\n",
    "            self.blocks.append(Block(n_channel,n_flow,affine,conv_lu))\n",
    "            n_channel*=2\n",
    "        self.blocks.append(Block(n_channel,n_flow,split=False,affine=affine))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        log_p_sum=0\n",
    "        logdet=0\n",
    "        out=input\n",
    "        z_outs=[]\n",
    "        for block in self.blocks:\n",
    "            out,det,log_p,z_new=block(out)\n",
    "            z_outs.append(z_new)\n",
    "            logdet=logdet+det\n",
    "            \n",
    "            if log_p is not None:\n",
    "                log_p_sum=log_p_sum+log_p\n",
    "        return log_p_sum,logdet,z_outs\n",
    "    \n",
    "    def reverse(self,z_list,reconstruct=False):\n",
    "        for i,block in enumerate(self.blocks[::-1]):\n",
    "            if i ==0:\n",
    "                input=block.reverse(z_list[-1],z_list[-1],reconstruct=reconstruct)\n",
    "                \n",
    "            else:\n",
    "                input=block.reverse(input,z_list[-(i+1)],reconstruct=reconstruct)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd8b3ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256]),\n",
       " tensor(2141.1357, grad_fn=<AddBackward0>),\n",
       " [torch.Size([256, 2, 16, 16]),\n",
       "  torch.Size([256, 4, 8, 8]),\n",
       "  torch.Size([256, 8, 4, 4]),\n",
       "  torch.Size([256, 32, 2, 2])])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glow=Glow(n_channel,n_flow,n_block)\n",
    "log_p_sum,logdet,z_outs = glow(data[0])\n",
    "log_p_sum.shape,logdet,[z.shape for z in z_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb5e5981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glow.reverse(z_samples).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76697f2f",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bd5c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glow=Glow(n_channel,n_flow,n_block)\n",
    "glow.to(device)\n",
    "optimizer=torch.optim.Adam(glow.parameters(),1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdd56c",
   "metadata": {},
   "source": [
    "# 损失函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0bcdd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logp,logdet,imagesize):\n",
    "    n_pixel = image_size * image_size * 1\n",
    "    loss=logdet+logp\n",
    "    return -loss.mean()/n_pixel,logp.mean()/n_pixel,logdet.mean()/n_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bfa6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_shapes(n_channel,input_size,n_flow,n_block):\n",
    "    z_shapes=[]\n",
    "    for i in range(n_block-1):\n",
    "        input_size//=2\n",
    "        n_channel*=2\n",
    "        z_shapes.append((n_channel,input_size,input_size))\n",
    "    input_size//=2\n",
    "    z_shapes.append((n_channel*4,input_size,input_size))\n",
    "    return z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits=5\n",
    "n_bins = 2.0 ** n_bits\n",
    "z_sample=[]\n",
    "z_shapes=calc_z_shapes(1,image_size,n_flow,n_block)\n",
    "for z in z_shapes:\n",
    "    z_new=torch.randn(n_sampls,*z)\n",
    "    z_sample.append(z_new.to(device))\n",
    "index=0\n",
    "EPOCHS=400\n",
    "for epoch in range(EPOCHS):\n",
    "    for data,label in mnist:\n",
    "        x=data.to(device)\n",
    "        x = x * 255\n",
    "        x = torch.floor(x / 2 ** (8 - n_bits))\n",
    "        x = x / n_bins - 0.5\n",
    "       \n",
    "        if index==0:\n",
    "            with torch.no_grad():\n",
    "                log_p,logdet,_=glow(x+ torch.rand_like(x) / n_bins)\n",
    "                index+=1\n",
    "                continue\n",
    "        else:\n",
    "            log_p,logdet,_=glow(x+torch.rand_like(x) / n_bins)\n",
    "            index+=1\n",
    "        logdet=logdet.mean()\n",
    "        \n",
    "        loss,log_p,log_det=calc_loss(log_p,logdet,image_size)\n",
    "        glow.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        if index%50==0:\n",
    "            print(f\"epoch {epoch} index : {index}, loss {loss} , logdet :{log_det}\")\n",
    "            with torch.no_grad():\n",
    "        \n",
    "                save_image(\n",
    "                glow.reverse(z_sample).cpu().data,\n",
    "                    f\"sample/{ str(index).zfill(6)}.png\",\n",
    "                    normalize=True,\n",
    "                    nrow=5,\n",
    "                    range=(-0.5, 0.5),\n",
    "                \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e93faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2d13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedb360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65464f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti",
   "language": "python",
   "name": "tti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
