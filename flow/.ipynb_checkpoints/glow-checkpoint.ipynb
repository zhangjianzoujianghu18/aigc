{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60cef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waj/anaconda3/envs/tts/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1 import package\n",
    "import torch\n",
    "import  torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg as la\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03191991",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "EPOCHS=200\n",
    "image_size=32\n",
    "channel=1\n",
    "z_dim=128\n",
    "n_flow=30\n",
    "n_block=4\n",
    "n_channel=1\n",
    "n_sampls=25\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c644753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jitter:\n",
    "    \"\"\"Transform for dataloader, adds uniform jitter noise to data\"\"\"\n",
    "\n",
    "    def __init__(self, scale=1.0 / 256):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "          scale: Scaling factor for noise\n",
    "        \"\"\"\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        eps = torch.rand_like(x) * self.scale\n",
    "        x_ = x + eps\n",
    "        return x_\n",
    "    \n",
    "class Scale:\n",
    "    \"\"\"Transform for dataloader, adds uniform jitter noise to data\"\"\"\n",
    "\n",
    "    def __init__(self, scale=255.0 / 256.0):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "          scale: Scaling factor for noise\n",
    "        \"\"\"\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7360efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mnist_image(image_array, label):\n",
    "    plt.imshow(image_array, cmap='Greys')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "# 1 data loader \n",
    "dataset=datasets.MNIST(\"../data/\",train=True,transform=transforms.Compose([\n",
    "    transforms.Resize(32),transforms.ToTensor()\n",
    "]))\n",
    "mnist=DataLoader(dataset,shuffle=True,batch_size=BATCH_SIZE,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab469bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in mnist:\n",
    "    print(data[0].shape)\n",
    "    show_mnist_image(data[0][0][0], data[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4613d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_shapes(n_channel,input_size,n_flow,n_block):\n",
    "    z_shapes=[]\n",
    "    for i in range(n_block-1):\n",
    "        input_size//=2\n",
    "        n_channel*=2\n",
    "        z_shapes.append((n_channel,input_size,input_size))\n",
    "    input_size//=2\n",
    "    z_shapes.append((n_channel*4,input_size,input_size))\n",
    "    return z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98084eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 16, 16), (4, 8, 8), (8, 4, 4), (32, 2, 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_shapes=calc_z_shapes(n_channel,image_size,n_flow,n_block)\n",
    "z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c15daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2, 16, 16])\n",
      "torch.Size([25, 4, 8, 8])\n",
      "torch.Size([25, 8, 4, 4])\n",
      "torch.Size([25, 32, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_samples=[]\n",
    "for z in z_shapes:\n",
    "    z_new=torch.randn(n_sampls,*z)\n",
    "    z_samples.append(z_new)\n",
    "[print(x.shape) for x in z_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382bc40",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608929d1",
   "metadata": {},
   "source": [
    "![架构图](./architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logabs=lambda x : torch.log(torch.abs(x))\n",
    "class Actnorm(nn.Module):\n",
    "    def __init__(self,in_channel,logdet=True):\n",
    "        super(Actnorm,self).__init__()\n",
    "        self.bias=nn.Parameter(torch.zeros(1,in_channel,1,1))\n",
    "        self.scale=nn.Parameter(torch.ones(1,in_channel,1,1))\n",
    "        self.register_buffer(\"initialized\",torch.tensor(0))\n",
    "        self.logdet=logdet\n",
    "    \n",
    "    # 求出channel维度的均值和方差作为初始值\n",
    "    def initialize(self,input):\n",
    "        with torch.no_grad():\n",
    "            flatten=input.permute(1,0,2,3).contiguous().view(input.shape[1],-1)\n",
    "            mean=flatten.mean(1).unsqueeze(1).unsqueeze(2).unsqueeze(3).permute(1,0,2,3)\n",
    "            std=flatten.std(1).unsqueeze(1).unsqueeze(2).unsqueeze(3).permute(1,0,2,3)\n",
    "            self.bias.data.copy_(-mean)\n",
    "            self.scale.data.copy_(1/(std+1e-6)) # 防止数据下益\n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        if self.initialized.item()==0:\n",
    "            self.initialize(input)\n",
    "            self.initialized.fill_(1)\n",
    "        \n",
    "        y=(input+self.bias)*self.scale\n",
    "        \n",
    "        if self.logdet:\n",
    "            logdet=height*width*torch.sum(logabs(self.scale))\n",
    "            return y,logdet\n",
    "        else :\n",
    "            return y\n",
    "        \n",
    "    def reverse(self,input):\n",
    "        return (input-self.bias)/self.scale\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0407787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 28, 28]) tensor(-4.3492, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "actorn=Actnorm(1)\n",
    "x=torch.randn(3,1,28,28)\n",
    "x,logdet=actorn(x)\n",
    "print(x.shape,logdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad88d3d",
   "metadata": {},
   "source": [
    "![架构图](./table3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1942a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvConv2d(nn.Module):\n",
    "    def __init__(self,in_channel):\n",
    "        super(InvConv2d,self).__init__()\n",
    "        self.weight=nn.Parameter(torch.randn(in_channel,in_channel,1,1))\n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        out=nn.functional.conv2d(input,self.weight)\n",
    "        logdet=height*width*torch.slogdet(self.weight.double())[1].float()\n",
    "        return out ,logdet\n",
    "    def reverse(self,input):\n",
    "        out=nn.functional.conv2d(input,self.weight.inverse().unsqueeze().unsqueeze(3))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec77b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8537557755519303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.06235623,  0.99803367, -0.00636407],\n",
       "        [-0.87524214, -0.05161767,  0.48092287],\n",
       "        [ 0.47964872,  0.03555864,  0.87673981]]),\n",
       " array([[ 1.58774139, -0.90916651, -0.35487224],\n",
       "        [ 0.        ,  2.57267047, -0.15919743],\n",
       "        [ 0.        ,  0.        , -0.00605003]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight=np.random.randn(3,3)\n",
    "q,d=la.qr(weight)\n",
    "print(sum(q[1,:]*q[:,1]))\n",
    "q,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9d434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.87524214 -0.05161767  0.48092287]\n",
      " [ 0.          1.00171114 -0.0406272 ]\n",
      " [ 0.          0.          1.14058925]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.87524214,  1.00171114,  1.14058925])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,l,u=la.lu(q)\n",
    "print(u)\n",
    "np.diag(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5840522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.05161767,  0.48092287],\n",
       "       [ 0.        ,  0.        , -0.0406272 ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(u,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4fef4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones_like(u),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e62e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_weight=np.random.randn(3,3)\n",
    "q,r=la.qr(r_weight)\n",
    "p,l,u=la.lu(q.astype(np.float32))\n",
    "s=np.diag(u)\n",
    "u=np.triu(u,1)\n",
    "\n",
    "u_mask=np.triu(np.ones_like(u),1)\n",
    "l_mask=u_mask.T\n",
    "l_eye=np.eye(l_mask.shape[0])\n",
    "# weight=p@(l*l_mask+l_eye)@(u*u_mask+torch.diag(s_sign*torch.exp(s)))\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de4ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.43694776 -0.26399288]\n",
      " [ 0.          0.          0.533548  ]\n",
      " [ 0.          0.          0.        ]] [2.3628647 2.4113631 3.7481303]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l*l_mask+l_eye\n",
    "print(u*u_mask,np.exp(s))\n",
    "u*u_mask+np.diag(np.exp(s))\n",
    "np.diag(np.exp(s)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6b9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvConv2d_lu(nn.Module):\n",
    "    def __init__(self,in_channel):\n",
    "        super(InvConv2d_lu,self).__init__()\n",
    "        weight=np.random.randn(in_channel,in_channel)\n",
    "        q,r=la.qr(weight)\n",
    "        p,l,u=la.lu(q.astype(np.float32))\n",
    "        s=np.diag(u)\n",
    "        u=np.triu(u,1)\n",
    "        u_mask=np.triu(np.ones_like(u),1)\n",
    "        l_mask=u_mask.T\n",
    "        \n",
    "        p=torch.from_numpy(p)\n",
    "        l=torch.from_numpy(l)\n",
    "        s=torch.from_numpy(s)\n",
    "        u=torch.from_numpy(u)\n",
    "        u_mask=torch.from_numpy(u_mask)\n",
    "        l_mask=torch.from_numpy(l_mask)\n",
    "        \n",
    "        self.register_buffer(\"p\",p)\n",
    "        self.register_buffer(\"u_mask\",u_mask)\n",
    "        self.register_buffer(\"l_mask\",l_mask)\n",
    "        self.register_buffer(\"s_sign\",torch.sign(s))\n",
    "        self.register_buffer(\"l_eye\",torch.eye(l_mask.shape[0]))\n",
    "        self.l=nn.Parameter(l)\n",
    "        self.s=nn.Parameter(logabs(s))\n",
    "        self.u=nn.Parameter(u)\n",
    "        \n",
    "    def calc_weight(self):\n",
    "        \n",
    "        weight=self.p@(self.l*self.l_mask+self.l_eye)@ \\\n",
    "        (self.u*self.u_mask+torch.diag(self.s_sign*torch.exp(self.s)))\n",
    "        return weight.unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        bs,channel,height,width=input.shape\n",
    "        weight=self.calc_weight()\n",
    "        out=nn.functional.conv2d(input,weight)\n",
    "        logdet=height*width*torch.sum(self.s)\n",
    "        return out ,logdet\n",
    "    \n",
    "    def reverse(self,input):\n",
    "        weight=self.calc_weight()\n",
    "        out=nn.functional.conv2d(input,weight.squeeze().inverse().unsqueeze(2).unsqueeze(3)) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c542e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 28, 28]) tensor(0., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55920/2649664392.py:14: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  s=torch.from_numpy(s)\n"
     ]
    }
   ],
   "source": [
    "invConv_lu=InvConv2d_lu(1)\n",
    "\n",
    "x,logdet=invConv_lu(x)\n",
    "print(x.shape,logdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39567d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invConv_lu.u_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06da9bc",
   "metadata": {},
   "source": [
    "![架构图](./table3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a4e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroConv2d(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,padding=1):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv2d(in_channel,out_channel,3,padding)\n",
    "        self.conv.weight.data.zero_()\n",
    "        self.conv.bias.data.zero_()\n",
    "        self.scale=nn.Parameter(torch.zeros(1,out_channel,1,1))\n",
    "    def forward(self,input):\n",
    "        out=nn.functional.pad(input,[1,1,1,1],value=1)\n",
    "        out=self.conv(out)\n",
    "        out=out*torch.exp(self.scale*3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f90222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroConv=ZeroConv2d(1,10)\n",
    "zeroConv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6501641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self,in_channel,filter_size=512,affine=True):\n",
    "        super(AffineCoupling,self).__init__()\n",
    "        self.affine=affine\n",
    "        self.net=nn.Sequential(\n",
    "        nn.Conv2d(in_channel//2,filter_size,3,padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(filter_size,filter_size,1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        ZeroConv2d(filter_size,in_channel if self.affine else in_channel//2)\n",
    "        )\n",
    "        \n",
    "        self.net[0].weight.data.normal_(0,0.05)\n",
    "        self.net[0].bias.data.zero_()\n",
    "        self.net[2].weight.data.normal_(0,0.05)\n",
    "        self.net[2].bias.data.zero_()     \n",
    "    def forward(self,input):\n",
    "        in_a,in_b=input.chunk(2,1)\n",
    "        if self.affine:\n",
    "            logs,t=self.net(in_b).chunk(2,1)\n",
    "            y_a=torch.exp(logs)*in_a+t\n",
    "            y_b=in_b\n",
    "            logdet=torch.sum(logabs(torch.exp(logs)))\n",
    "        else:\n",
    "            y_b=in_b\n",
    "            y_a=in_a+self.net(in_b)\n",
    "            logdet=None\n",
    "#         return y_a,y_b\n",
    "        return torch.cat([y_a,y_b],1),logdet\n",
    "    def reverse(self,output):\n",
    "        y_a,y_b=output.chunk(2,1)\n",
    "        if self.affine:\n",
    "            in_b=y_b\n",
    "            logs,t=self.net(in_b).chunk(2, 1)\n",
    "            in_a=(y_a-t)/torch.exp(logs)\n",
    "        else :\n",
    "            in_b=y_b\n",
    "            in_a=self.net(in_b)-y_a\n",
    "        return torch.cat([in_a,in_b],1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac2765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 28, 28]), tensor(0., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affCoup=AffineCoupling(4)\n",
    "a,b=affCoup(torch.randn(3,4,28,28))\n",
    "\n",
    "a.shape,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41428b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0299, 0.9765])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(torch.randn(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f802b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self,in_channel,affine=True,conv_lu=True):\n",
    "        super(Flow,self).__init__()\n",
    "        self.actorm=Actnorm(in_channel)\n",
    "        if conv_lu:\n",
    "            self.invConv2d=InvConv2d_lu(in_channel)\n",
    "        else:\n",
    "            self.invConv2d=InvConv2d(in_channel)\n",
    "        self.coupling=AffineCoupling(in_channel,256)\n",
    "    def forward(self,input):\n",
    "        out,det1=self.actorm(input)\n",
    "        out,det2=self.invConv2d(out)\n",
    "        out,det3=self.coupling(out)\n",
    "        logdet=det1+det2\n",
    "        if det3 is not None:\n",
    "            logdet+=det3\n",
    "        return out,logdet\n",
    "    \n",
    "    def reverse(self,output):\n",
    "        input_=self.coupling.reverse(output)\n",
    "        input_=self.invConv2d.reverse(input_)\n",
    "        input_=self.actorm.reverse(input_)\n",
    "        return input_\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "513d92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 14, 14]), tensor(2.3917, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow=Flow(4)\n",
    "a,b=flow(torch.randn(3,4,14,14))\n",
    "a.shape,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb9303",
   "metadata": {},
   "source": [
    "![架构图](./architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be752def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_p(x,mean,log_std):\n",
    "    return -0.5*math.log(2* math.pi)-log_std-0.5*(x-mean)**2/torch.exp(2*log_std)\n",
    "\n",
    "\n",
    "def gaussian_sample(eps,mean,log_std):\n",
    "    return mean+torch.exp(log_std)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "928fdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,in_channel,n_flow,split=True,affine=True,conv_lu=True):\n",
    "        super().__init__()\n",
    "        squeeze_dim=in_channel*4\n",
    "        self.flows=nn.ModuleList()\n",
    "        for i in range(n_flow):\n",
    "            self.flows.append(Flow(squeeze_dim,affine=affine,conv_lu=conv_lu))\n",
    "        self.split=split\n",
    "        \n",
    "        if split:\n",
    "            self.prior=ZeroConv2d(in_channel*2,in_channel*4)\n",
    "        else: \n",
    "            self.prior=ZeroConv2d(in_channel*4,in_channel*8)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        bs,n_channel,height,width=input.shape\n",
    "        squeezed=input.view(bs,n_channel,height//2,2,width//2,2)\n",
    "        squeezed=squeezed.permute(0,1,3,5,2,4)\n",
    "        out=squeezed.contiguous().view(bs,n_channel*4,height//2,width//2)\n",
    "        logdet=0\n",
    "\n",
    "        for flow in self.flows:\n",
    "            out,det=flow(out)\n",
    "            logdet+=det\n",
    "        \n",
    "        if self.split:\n",
    "            out,z_new=out.chunk(2,1)\n",
    "            mean,logstd=self.prior(out).chunk(2,1)\n",
    "            log_p=gaussian_log_p(out,mean,logstd)\n",
    "            log_p=log_p.view(bs,-1).sum(1)\n",
    "            \n",
    "        else:\n",
    "            zero=torch.zeros_like(out)\n",
    "            mean,logstd=self.prior(zero).chunk(2,1)\n",
    "            log_p=gaussian_log_p(out,mean,logstd)\n",
    "            log_p=log_p.view(bs,-1).sum(1)\n",
    "            z_new=out\n",
    "        return out,logdet,log_p,z_new\n",
    "    \n",
    "    def reverse(self,output,eps=None,reconstruct=False):\n",
    "        input=output\n",
    "        \n",
    "        if reconstruct:\n",
    "            if self.split:\n",
    "                input=torch.cat([output,eps],1)\n",
    "            else:\n",
    "                input=eps\n",
    "        else:\n",
    "            if self.split:\n",
    "                mean,logstd=self.prior(input).chunk(2,1)\n",
    "                z=gaussian_sample(eps,mean,logstd)\n",
    "                input=torch.cat([output,z],1)\n",
    "                \n",
    "            else:\n",
    "                zero=torch.zeros_like(input)\n",
    "                mean,logstd=self.prior(zero).chunk(2,1)\n",
    "                z=gaussian_sample(eps,mean,logstd)\n",
    "                input=z\n",
    "        for flow in self.flows[::-1]:\n",
    "            input=flow.reverse(input)\n",
    "            \n",
    "        bs,n_channel,height,width=input.shape\n",
    "        unsqueezed=input.view(bs,n_channel//4,2,2,height,width)\n",
    "        unsqueezed=unsqueezed.permute(0,1,4,2,5,3)\n",
    "        unsqueezed=unsqueezed.contiguous().view(bs,n_channel//4,height*2,width*2)\n",
    "        \n",
    "        return unsqueezed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f56f0eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 14, 14]),\n",
       " tensor(-9.0875, grad_fn=<AddBackward0>),\n",
       " tensor([-551.6480, -554.0338, -555.1552], grad_fn=<SumBackward1>),\n",
       " torch.Size([3, 2, 14, 14]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block=Block(1,2)\n",
    "out,logdet,log_p,z_new=block(torch.randn(3,1,28,28))\n",
    "out.shape,logdet,log_p,z_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f81b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior=ZeroConv2d(1*2,1*4)\n",
    "prior(torch.zeros(1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64cbc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glow(nn.Module):\n",
    "    def __init__(self,in_channel,n_flow,n_block,affine=True,conv_lu=True):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList()\n",
    "        n_channel=in_channel\n",
    "        for i in range(n_block-1):\n",
    "            self.blocks.append(Block(n_channel,n_flow,affine,conv_lu))\n",
    "            n_channel*=2\n",
    "        self.blocks.append(Block(n_channel,n_flow,split=False,affine=affine))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        log_p_sum=0\n",
    "        logdet=0\n",
    "        out=input\n",
    "        z_outs=[]\n",
    "        for block in self.blocks:\n",
    "            out,det,log_p,z_new=block(out)\n",
    "            z_outs.append(z_new)\n",
    "            logdet=logdet+det\n",
    "            \n",
    "            if log_p is not None:\n",
    "                log_p_sum=log_p_sum+log_p\n",
    "        return log_p_sum,logdet,z_outs\n",
    "    \n",
    "    def reverse(self,z_list,reconstruct=False):\n",
    "        for i,block in enumerate(self.blocks[::-1]):\n",
    "            if i ==0:\n",
    "                input=block.reverse(z_list[-1],z_list[-1],reconstruct=reconstruct)\n",
    "                \n",
    "            else:\n",
    "                input=block.reverse(input,z_list[-(i+1)],reconstruct=reconstruct)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd8b3ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256]),\n",
       " tensor(2138.2490, grad_fn=<AddBackward0>),\n",
       " [torch.Size([256, 2, 16, 16]),\n",
       "  torch.Size([256, 4, 8, 8]),\n",
       "  torch.Size([256, 8, 4, 4]),\n",
       "  torch.Size([256, 32, 2, 2])])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glow=Glow(n_channel,n_flow,n_block)\n",
    "log_p_sum,logdet,z_outs = glow(data[0])\n",
    "log_p_sum.shape,logdet,[z.shape for z in z_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb5e5981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0092, -0.1461, -0.1222,  ..., -0.0801,  0.6069,  0.6094],\n",
       "          [-0.1198, -0.2296, -0.2038,  ..., -0.3971,  0.6273,  0.4009],\n",
       "          [-0.1194, -0.1840,  0.0813,  ...,  0.4662,  0.4477, -0.0240],\n",
       "          ...,\n",
       "          [ 0.3498,  0.3995,  0.4003,  ...,  0.1114,  0.5412,  0.3976],\n",
       "          [ 0.7509,  0.7893,  0.0171,  ..., -0.1114,  0.5454,  0.2837],\n",
       "          [ 0.6350,  0.6051,  0.0071,  ..., -0.1497,  0.5449,  0.3159]]],\n",
       "\n",
       "\n",
       "        [[[-0.0287,  0.0684,  0.1607,  ...,  0.2297,  0.2054, -0.1111],\n",
       "          [ 0.0803,  0.0906,  0.2935,  ...,  0.2281,  0.0389, -0.2083],\n",
       "          [ 0.0462, -0.1441,  0.1434,  ..., -0.3742,  0.5466,  0.6132],\n",
       "          ...,\n",
       "          [-0.2792, -0.1184, -0.3212,  ..., -0.5314,  0.3228,  0.3883],\n",
       "          [ 0.1396, -0.0015,  0.2951,  ..., -0.1277, -0.2007, -0.2213],\n",
       "          [ 0.0754, -0.0912,  0.4376,  ..., -0.0491, -0.1486, -0.1754]]],\n",
       "\n",
       "\n",
       "        [[[-0.0713, -0.1787,  0.1764,  ..., -0.0458,  0.2741,  0.2124],\n",
       "          [-0.2731, -0.2565,  0.1434,  ..., -0.1310, -0.0113, -0.0616],\n",
       "          [-0.1583, -0.0493,  0.2727,  ...,  0.2367, -0.0668,  0.1248],\n",
       "          ...,\n",
       "          [ 0.1265, -0.0942,  0.3284,  ...,  0.0575, -0.4264, -0.3090],\n",
       "          [-0.3207, -0.6465,  0.4234,  ...,  0.0841, -0.4122, -0.3748],\n",
       "          [-0.2074, -0.4186,  0.4901,  ..., -0.1272, -0.5055, -0.5064]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.7974,  0.7781,  0.2503,  ...,  0.1870, -0.1202,  0.1569],\n",
       "          [ 0.9278,  0.8170, -0.0265,  ...,  0.1599, -0.1231, -0.0163],\n",
       "          [ 0.3158,  0.2765,  0.0478,  ..., -0.1615, -0.2184, -0.1745],\n",
       "          ...,\n",
       "          [ 0.0186,  0.1190,  0.0105,  ..., -0.1168, -0.1282,  0.0369],\n",
       "          [ 0.1089,  0.0967,  0.0243,  ..., -0.4367, -0.3197, -0.2308],\n",
       "          [ 0.2760,  0.3615,  0.0223,  ..., -0.2627, -0.2917, -0.2759]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1782,  0.3857,  0.1434,  ..., -0.0857,  0.0405,  0.0247],\n",
       "          [ 0.2709,  0.5538,  0.1648,  ...,  0.0032,  0.0905,  0.0571],\n",
       "          [ 0.1041,  0.2508,  0.1639,  ..., -0.3380, -0.3786, -0.3123],\n",
       "          ...,\n",
       "          [-0.0111, -0.0320, -0.3854,  ...,  0.0964, -0.1806, -0.1243],\n",
       "          [-0.1490, -0.2433, -0.1802,  ...,  0.1016, -0.1037, -0.3391],\n",
       "          [-0.1253, -0.1344, -0.1453,  ..., -0.0991, -0.3270, -0.5200]]],\n",
       "\n",
       "\n",
       "        [[[-0.3103, -0.2414, -0.2600,  ..., -0.0770, -0.1828, -0.1685],\n",
       "          [-0.2522, -0.2089, -0.2692,  ...,  0.1244, -0.3532, -0.4559],\n",
       "          [-0.1947, -0.0314, -0.0616,  ..., -0.3971,  0.0063, -0.0970],\n",
       "          ...,\n",
       "          [-0.1714, -0.0943,  0.5661,  ...,  0.8430,  0.0537, -0.0309],\n",
       "          [-0.2278, -0.2292,  0.1216,  ...,  0.0501, -0.0688, -0.1251],\n",
       "          [-0.2771, -0.4258,  0.1458,  ...,  0.2987, -0.1033,  0.0140]]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glow.reverse(z_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76697f2f",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bd5c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glow=Glow(n_channel,n_flow,n_block)\n",
    "glow.to(device)\n",
    "optimizer=torch.optim.Adam(glow.parameters(),1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdd56c",
   "metadata": {},
   "source": [
    "# 损失函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0bcdd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logp,logdet,imagesize):\n",
    "    n_pixel = image_size * image_size * 1\n",
    "    loss=logdet+logp\n",
    "    return -loss.mean()/n_pixel,logp.mean()/n_pixel,logdet.mean()/n_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bfa6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_shapes(n_channel,input_size,n_flow,n_block):\n",
    "    z_shapes=[]\n",
    "    for i in range(n_block-1):\n",
    "        input_size//=2\n",
    "        n_channel*=2\n",
    "        z_shapes.append((n_channel,input_size,input_size))\n",
    "    input_size//=2\n",
    "    z_shapes.append((n_channel*4,input_size,input_size))\n",
    "    return z_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad6b22fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mn_bits\n\u001b[1;32m      3\u001b[0m z_sample\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      4\u001b[0m z_shapes\u001b[38;5;241m=\u001b[39mcalc_z_shapes(\u001b[38;5;241m1\u001b[39m,image_size,n_flow,n_block)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "n_bits=5\n",
    "n_bins = 2.0 ** args.n_bits\n",
    "z_sample=[]\n",
    "z_shapes=calc_z_shapes(1,image_size,n_flow,n_block)\n",
    "for z in z_shapes:\n",
    "    z_new=torch.randn(n_sampls,*z)\n",
    "    z_sample.append(z_new.to(device))\n",
    "index=0\n",
    "EPOCHS=100\n",
    "for epoch in range(EPOCHS):\n",
    "    for data,label in mnist:\n",
    "        x=data.to(device)\n",
    "        x = x * 255\n",
    "        x = torch.floor(x / 2 ** (8 - n_bits))\n",
    "        x = x / n_bins - 0.5\n",
    "       \n",
    "        if index==0:\n",
    "            with torch.no_grad():\n",
    "                log_p,logdet,_=glow(x+ torch.rand_like(x) / n_bins)\n",
    "                index+=1\n",
    "                continue\n",
    "        else:\n",
    "            log_p,logdet,_=glow(x+torch.rand_like(x) / n_bins)\n",
    "            index+=1\n",
    "        logdet=logdet.mean()\n",
    "        \n",
    "        loss,log_p,log_det=calc_loss(log_p,logdet,image_size)\n",
    "        glow.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        if index%50==0:\n",
    "            print(f\"epoch {epoch} index : {index}, loss {loss} , logdet :{log_det}\")\n",
    "            with torch.no_grad():\n",
    "                os.makedirs(\"sample\",exist_ok=True)\n",
    "                save_image(\n",
    "                glow.reverse(z_sample).cpu().data,\n",
    "                    f\"sample/{ str(index).zfill(6)}.png\",\n",
    "                    normalize=True,\n",
    "                    nrow=5,\n",
    "                \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e93faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2d13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedb360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65464f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
